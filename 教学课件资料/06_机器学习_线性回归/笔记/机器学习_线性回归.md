# 人工智能历史

## 图灵测试

测试者与被测试者（一个人和一台机器）隔开的情况下，通过一些装置（如键盘）向被测试者随意提问。多次测试（一般为 $5$ 分钟之内），如果有超过 $30\%$ 的测试者不能确定被测试者是人还是机器，那么这台机器就通过了测试，并被认为具有人类智能。

> ![image-20241008152915581](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/image-20241008152915581.png)

## 达特茅斯会议

$1956$ 年 $8$ 月，在美国汉诺斯小镇宁静的达特茅斯学院中，

- 约翰· 麦卡锡（John McCarthy）

- 马文· 闵斯基（Marv in Minsky，人工智能与认知学专家）
- 克劳德· 香农（Claude Shannon，信息论的创始人）
- 艾伦· 纽厄尔（Allen Newell，计算机科学家）
- 赫伯特· 西蒙（Herbert Simon，诺贝尔经济学奖得主

等科学家正聚在一起，讨论着一歌完全不食人间烟火的主题：用机器来模仿人类学习以及其他方面的智能。会议足足开了两个月的时间，虽然大家没有达成普遍的共识，但是却为会议讨论的内容起了一个名字：人工智能。因此，$1956$ 年也就成为了人工智能元年。

> ![image-20241008153126387](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/image-20241008153126387.png)

2006年，会议五十年后，当事人重聚达特茅斯。左起:摩尔，麦卡锡，明斯基，赛弗里奇，所罗门诺夫

> ![image-20241008153658180](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/image-20241008153658180.png)

## 发展历程

人工智能充满未知的探索道路曲折起伏。如何描述人工智能自 $1956$ 年以来 $60$ 余年的发展历程，学术界可谓仁者见仁、智者见智。我们将人工智能的发展历程划分为以下 $6$ 个阶段：

算力，海量数据，算法

> 第一是起步发展期：$1956$ 年 — $20$ 世纪 $60$ 年代初。
> 人工智能概念提出后，相继取得了一批令人瞩目的研究成果，如机器定理证明、跳棋程序等，掀起人工智能发展的第一个高潮。
>
> 第二是反思发展期：$20$ 世纪 $60$ 年代 — $70$ 年代初。
> 人工智能发展初期的突破性进展大大提升了人们对人工智能的期望，人们开始尝试更具挑战性的任务，并提出了一些不切实际的研发目标。然而，接二连三的失败和预期目标的落空（例如，无法用机器证明两个连续函数之和还是连续函数、机器翻译闹出笑话等），使人工智能的发展走入低谷。
>
> 第三是应用发展期：$20$ 世纪 $70$ 年代初 — $80$ 年代中。
> $20$ 世纪 $70$ 年代出现的专家系统模拟人类专家的知识和经验解决特定领域的问题，实现了人工智能从理论研究走向实际应用、从一般推理策略探讨转向运用专门知识的重大突破。专家系统在医疗、化学、地质等领域取得成功，推动⼈工智能走入应用发展的新高潮。
>
> 第四是低迷发展期：$20$ 世纪 $80$ 年代中 — $90$ 年代中。
>
> 随着人工智能的应用规模不断扩大，专家系统存在的应用领域狭窄、缺乏常识性知识、知识获取困难、推理方法单一、缺乏分布式功能、难以与现有数据库兼容等问题逐渐暴露出来。
>
> 第五是稳步发展期：$20$ 世纪 $90$ 年代中 — $2010$ 年。
> 由于网络技术特别是互联网技术的发展，加速了人工智能的创新研究，促使人工智能技术进一步走向实用化。$1997$ 年国际商业机器公司（简称 IBM）深蓝超级计算机战胜了国际象棋世界冠军卡斯帕罗夫，2008 年 IBM 提出“ 智慧地球” 的概念。以上都是这一时期的标志性事件。
>
> 第六是蓬勃发展期：$2011$ 年至今。
> 随着大数据、云计算、互联网、物联网等信息技术的发展，广泛的感知数据和图形处理器等计算平台，推动以深度神经网络为代表的人工智能技术飞速发展，大幅跨越了科学与应用之间的“ 技术鸿沟” ，诸如图像分类、语音识别、知识问答、人机对弈、无人驾驶等人工智能技术实现了从“ 不能用、不好用” 到“ 可以用” 的技术突破，迎来爆发式增长的新高潮。

> ![image-20241008154348664](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/image-20241008154348664.png)

# 人工智能概述

**人工智能**（英语：artificial intelligence，缩写为 AI）亦称智械、机器智能，指由人制造出来的机器所表现出来的智能。通常人工智能是指通过普通计算机程序来呈现人类智能的技术。该词也指出研究这样的智能系统是否能够实现，以及如何实现。

> bilibili.com/video/BV1M84y1G7PX

## 应用场景

人工智能目前在电脑领域内，得到了愈加广泛的发挥。并在机器人、自动驾驶、网络安全、电子商务、计算模拟、交通、社交网络中得到应用。

![机器人](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/tesla_robot1-1688199288896.png)

 ![机器人](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/tesla_robot2-1688199288896.gif)

![自动驾驶](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B61-1688199288896.png)

![自动驾驶](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B62-1688199288896.png)

## 人工智能发展三要素

- 数据

- 算法

- 计算力

  > CPU：适合需要前后计算步骤严密关联的计算场景。这些任务涉及到“流”的问题，必须先计算完第一步，再去计算第二步；比如你去相亲，双方看着顺眼才能继续发展。总不能你这边还没见面呢，那边找人把证都给领了。这种比较复杂的问题都是 CPU 来做的，生活中大部分用的都是 CPU，例如武器装备运动控制、个人电脑使用等。
  > GPU：适合前后计算步骤无依赖性，相互独立的计算场景。很多涉及到大量计算的问题基本都有这种特性，比如你说的破解密码，挖矿和很多图形学的计算。这些计算可以分解为多个相同的简单小任务，每个任务就可以分给一个小学生去做。
  >
  > 更多参考：https://zhuanlan.zhihu.com/p/156171120

## 人工智能主要分支

通讯、感知与行动是现代人工智能的三个关键能力，在这里我们将根据这些能力对这三个技术领域进行介绍：

- 计算机视觉（CV）
- 自然语言处理（NLP）
- 机器人

### 计算机视觉

**计算机视觉**（Computer vision）是一门研究如何使机器看的科学，更进一步的说，就是指用摄影机和计算机代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图像处理，用计算机处理成为更适合人眼观察或传送给仪器检测的图像。

![计算机视觉](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89.png)

### 自然语言处理

**自然语言处理**（英语：Natural Language Processing，缩写作 **NLP**）是人工智慧和语言学领域的分支学科。此领域探讨如何处理及运用自然语言；自然语言处理包括多方面和步骤，基本有认知、理解、生成等部分。

自然语言认知和理解是让电脑把输入的语言变成有意思的符号和关系，然后根据目的再处理。自然语言生成系统则是把电脑数据转化为自然语言。

![自然语言处理](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.png)

ChatGPT 是一个基于 NLP 技术的语言模型，它采用了 GPT（Generative Pre-trained Transformer，生成式预训练变换器）的架构。GPT 是一种深度学习模型，通过在大规模文本数据上进行预训练，学习了语言的统计规律和语义表示，从而能够生成具有连贯性和合理性的文本回复。

总结来说，NLP 是一个研究领域，涉及到处理自然语言的各种任务，而 ChatGPT 是基于 NLP 技术构建的具体应用，用于实现智能对话和文本生成。

### 机器人

**机器人**（英语：Robot）包括一切模拟人类行为或思想与模拟其他生物的机械（如机器狗、机器猫等）。狭义上对机器人的定义还有很多分类法及争议，有些电脑程序甚至也被称为机器人。在当代工业中，机器人指能自动执行任务的人造机器设备，用以取代或协助人类工作，一般会是机电设备，由计算机程序或是电子电路控制。

![机器人](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E6%9C%BA%E5%99%A8%E4%BA%BA.png)

## 人工智能、机器学习、深度学习是什么关系？

1956 年提出 AI 概念，短短 3 年后（1959） [Arthur Samuel](https://en.wikipedia.org/wiki/Arthur_Samuel) 就提出了机器学习的概念：

> Field of study that gives computers the ability to learn without being explicitly programmed.
>
> 机器学习研究和构建的是一种特殊算法（**而非某一个特定的算法**），能够让计算机自己在数据中学习从而进行预测。

所以，**机器学习不是某种具体的算法，而是很多算法的统称。**

机器学习包含了很多种不同的算法，深度学习就是其中之一，其他方法包括决策树，聚类，贝叶斯等。

深度学习的灵感来自大脑的结构和功能，即许多神经元的互连。人工神经网络（ANN）是模拟大脑生物结构的算法。

不管是机器学习还是深度学习，都属于人工智能（AI）的范畴。所以人工智能、机器学习、深度学习可以用下面的图来表示：

![机器学习、人工智能、深度学习的关系](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/ai-ml-dl-1688199288897.png)

# 机器学习

## 什么是机器学习

在解释机器学习的原理之前，先把最精髓的基本思路介绍给大家，理解了机器学习最本质的东西，就能更好的利用机器学习，同时这个解决问题的思维还可以用到工作和生活中。

### 机器学习的基本思路

- 把现实生活中的问题抽象成数学模型，并且很清楚模型中不同参数的作用。

- 利用数学方法对这个数学模型进行求解，从而解决现实生活中的问题。

- 评估这个数学模型，是否真正的解决了现实生活中的问题，解决的如何？

**无论使用什么算法，使用什么样的数据，最根本的思路都逃不出上面的 3 步！**

![机器学习](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/ml-1689808656966.png)

当我们理解了这个基本思路，我们就能发现：

不是所有问题都可以转换成数学问题的。那些没有办法转换的现实问题 AI 就没有办法解决。同时最难的部分也就是把现实问题转换为数学问题这一步。

### 机器学习的原理

下面以监督学习为例，给大家讲解一下机器学习的实现原理。

假如我们正在教小朋友识字（一、二、三）。我们首先会拿出 3 张卡片，然后便让小朋友看卡片，一边说“一条横线的是一、两条横线的是二、三条横线的是三”。

![机器学习原理说明1](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/ml-step-1-1689808656967.png)

不断重复上面的过程，小朋友的大脑就在不停的学习。

![机器学习原理说明2](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/ml-step-2-1689808656967.png)

当重复的次数足够多时，小朋友就学会了一个新技能，认识汉字：一、二、三。

![机器学习原理说明3](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/ml-step-3-1689808656967.png)

我们用上面人类的学习过程来类比机器学习。机器学习跟上面提到的人类学习过程很相似。

- 上面提到的认字的卡片在机器学习中叫——训练集
- 上面提到的“一条横线，两条横线”这种区分不同汉字的属性叫——特征
- 小朋友不断学习的过程叫——建模
- 学会了识字后总结出来的规律叫——模型

**通过训练集，不断识别特征，不断建模，最后形成有效的模型，这个过程就叫“机器学习”！**

![机器学习原理说明4](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/ml-step-4-1689808656968.png)

 ## 机器学习分类

机器学习根据训练方法大致可以分为 3 大类：

- 监督学习

- 非监督学习
- 强化学习

除此之外，大家可能还听过“半监督学习”之类的说法，但是那些都是基于上面 3 类的变种，本质没有改变。

### 监督学习

监督学习是指我们给算法一个数据集，并且给定正确答案。机器通过数据来学习正确答案的计算方法。

举个栗子：

我们准备了一大堆猫和狗的照片，我们想让机器学会如何识别猫和狗。当我们使用监督学习的时候，我们需要给这些照片打上标签。

![将打好标签的照片用来训练](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/tag-cat-dog-1689808656968.png)

我们给照片打的标签就是“正确答案”，机器通过大量学习，就可以学会在新照片中认出猫和狗。

![当机器遇到新的小狗照片时就能认出他](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/dog-1689808656968.png)

这种通过大量人工打标签来帮助机器学习的方式就是监督学习。这种学习方式效果非常好，但是成本也非常高。

### 非监督学习

非监督学习中，给定的数据集没有“正确答案”，所有的数据都是一样的。无监督学习的任务是从给定的数据集中，挖掘出潜在的结构。

举个栗子：

我们把一堆猫和狗的照片给机器，不给这些照片打任何标签，但是我们希望机器能够将这些照片分分类。

![将不打标签的照片给机器](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/hun-1689808656968.png)

通过学习，机器会把这些照片分为 2 类，一类都是猫的照片，一类都是狗的照片。虽然跟上面的监督学习看上去结果差不多，但是有着本质的差别：

**非监督学习中，虽然照片分为了猫和狗，但是机器并不知道哪个是猫，哪个是狗。对于机器来说，相当于分成了 A、B 两类。**

![机器可以将猫和狗分开，但是并不知道哪个是猫，哪个是狗](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/fenlei-1689808656968.png)

### 强化学习

强化学习更接近生物学习的本质，因此有望获得更高的智能。它关注的是智能体如何在环境中采取一系列行为，从而获得最大的累积回报。通过强化学习，一个智能体应该知道在什么状态下应该采取什么行为。

最典型的场景就是打游戏。

2019 年 1 月 25 日，AlphaStar（Google 研发的人工智能程序，采用了强化学习的训练方式） 完虐星际争霸的职业选手职业选手“TLO”和“MANA”。

> https://www.bilibili.com/video/BV1tE411Q74p 捉迷藏游戏

## 机器学习实操的 7 个步骤

通过上面的内容，我们对机器学习已经有一些模糊的概念了，这个时候肯定会特别好奇：到底怎么使用机器学习？

机器学习在实际操作层面一共分为 7 步：

1. 收集数据
2. 数据准备
3. 选择一个模型  线性回归, 逻辑回归
4. 训练  训练对应的参数 y=wx+b  在添加 激活函数: Sigmoid 函数, Relu函数, Tanh函数
5. 评估: 新的数据, 使用训练好的模型去运行, 评估指标:  线性回归: loss损失函数:  MSE: 均方误差
6. 参数调整
7. 预测（开始使用）

![机器学习的7个步骤](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/7steps-ml-1689808656968.png)

假设我们的任务是通过酒精度和颜色来区分红酒和啤酒，下面详细介绍一下机器学习中每一个步骤是如何工作的。

![案例目标：区分红酒和啤酒](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/wine-beer-1689808656968.png)

### 收集数据

我们在超市买来一堆不同种类的啤酒和红酒，然后再买来测量颜色的光谱仪和用于测量酒精度的设备。

这个时候，我们把买来的所有酒都标记出他的颜色和酒精度，会形成下面这张表格。

| 颜色 | 酒精度 | 种类 |
| :--- | :----- | :--- |
| 610  | 5      | 啤酒 |
| 599  | 13     | 红酒 |
| 693  | 14     | 红酒 |
| …    | …      | …    |

**这一步非常重要，因为数据的数量和质量直接决定了预测模型的好坏。**

### 数据准备

在这个例子中，我们的数据是很工整的，但是在实际情况中，我们收集到的数据会有很多问题，所以会涉及到数据清洗等工作。

当数据本身没有什么问题后，我们将数据分成 3 个部分：训练集（60%）、验证集（20%）、测试集（20%），用于后面的验证和评估工作。

![数据要分为3个部分：训练集、验证集、测试集](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/3dataset-1689808656968.png)

### 选择一个模型

研究人员和数据科学家多年来创造了许多模型。有些非常适合图像数据，有些非常适合于序列（如文本或音乐），有些用于数字数据，有些用于基于文本的数据。

在我们的例子中，由于我们只有 2 个特征，颜色和酒精度，我们可以使用一个小的线性模型，这是一个相当简单的模型。

### 训练

大部分人都认为这个是最重要的部分，其实并非如此~ 数据数量和质量、还有模型的选择比训练本身重要更多（训练知识台上的 3 分钟，更重要的是台下的 10 年功）。

这个过程就不需要人来参与的，机器独立就可以完成，整个过程就好像是在做算术题。因为机器学习的本质就是**将问题转化为数学问题，然后解答数学题的过程**。

### 评估

一旦训练完成，就可以评估模型是否有用。这是我们之前预留的验证集和测试集发挥作用的地方。评估的指标主要有准确率。

这个过程可以让我们看到模型如何对尚未看到的数是如何做预测的。这意味着代表模型在现实世界中的表现。

### 参数调整

完成评估后，您可能希望了解是否可以以任何方式进一步改进训练。我们可以通过调整参数来做到这一点。当我们进行训练时，我们隐含地假设了一些参数，我们可以通过认为的调整这些参数让模型表现的更出色。

### 预测

我们上面的 6 个步骤都是为了这一步来服务的。这也是机器学习的价值。这个时候，当我们买来一瓶新的酒，只要告诉机器他的颜色和酒精度，他就会告诉你，这时啤酒还是红酒了。

## 4、经典机器学习算法

不同算法解决不同机器学习的问题。

| 算法           | 训练方式   |
| :------------- | :--------- |
| 线性回归       | 监督学习   |
| 逻辑回归       | 监督学习   |
| 线性判别分析   | 监督学习   |
| 决策树         | 监督学习   |
| 朴素贝叶斯     | 监督学习   |
| K 邻近         | 监督学习   |
| 学习向量量化   | 监督学习   |
| 支持向量机     | 监督学习   |
| 随机森林       | 监督学习   |
| AdaBoost       | 监督学习   |
| 高斯混合模型   | 非监督学习 |
| 限制波尔兹曼机 | 非监督学习 |
| K-means 聚类   | 非监督学习 |
| 最大期望算法   | 非监督学习 |

# 机器学习算法

## 线性回归（Liner Regression）

![线性回归](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png)

线性回归的位置如上图所示，它属于机器学习 – 监督学习 – 回归 – 线性回归。

### 什么是回归

回归的目的是为了预测，比如预测明天的天气温度，预测股票的走势…

回归之所以能预测是因为他通过历史数据，摸透了“套路”，然后通过这个套路来预测未来的结果。

![回归的底层逻辑](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E5%9B%9E%E5%BD%92.png)

 

### 什么是线性

“越…，越…”符合这种说法的就可能是线性关系： 等比例变化关系

「汉堡」买的越多，花的「钱」就越多

 杯子里的「水」越多，「重量」就越大

……

但是并非所有“越…，越…”都是线性的，比如“充电越久，电量越高”，他就类似下面的非线性曲线：

![充电时间和电量是非线性关系](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E9%9D%9E%E7%BA%BF%E6%80%A7.png)

线性关系不仅仅只能存在 2 个变量（二维平面）。3 个变量时（三维空间），线性关系就是一个平面，4 个变量时（四维空间），线性关系就是一个体。以此类推…

![线性关系可以是多个变量](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7.png)

### 什么是线性回归

线性回归本来是是统计学里的概念，现在经常被用在机器学习中。

如果 2 个或者多个变量之间存在“线性关系”，那么我们就可以通过历史数据，摸清变量之间的“套路”，建立一个有效的模型，来预测未来的变量结果。

![通俗解释线性回归](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E8%A7%A3%E9%87%8A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png)

### 线性回归的优缺点

优点：

1. 建模速度快，不需要很复杂的计算，在数据量大的情况下依然运行速度很快。
2. 可以根据系数给出每个变量的理解和解释

缺点：不能很好地拟合非线性数据。所以需要先判断变量之间是否是线性关系。

> 为什么在深度学习大杀四方的今天还使用线性回归呢？
>
> 一方面，线性回归所能够模拟的关系其实远不止线性关系。线性回归中的“线性”指的是系数的线性，而通过对特征的非线性变换，以及广义线性模型的推广，输出和特征之间的函数关系可以是高度非线性的。另一方面，也是更为重要的一点，线性模型的易解释性使得它在物理学、经济学、商学等领域中占据了难以取代的地位。

## 逻辑回归（Logistic Regression）

### 什么是逻辑回归

![逻辑回归在机器学习中的位置](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.png)

逻辑回归的位置如上图所示，它属于机器学习 – 监督学习 – 分类 – 逻辑回归。

**逻辑回归（Logistic Regression）主要解决二分类问题，用来表示某件事情发生的可能性**。

比如：

- 一封邮件是垃圾邮件的可能性（是、不是）
- 你购买一件商品的可能性（买、不买）
- 广告被点击的可能性（点、不点）

### 逻辑回归的优缺点

**优点：**

- 实现简单，广泛的应用于工业问题上；
- 分类时计算量非常小，速度很快，存储资源低；
- 便于观测样本概率分数；
- 对逻辑回归而言，多重共线性并不是问题，它可以结合 L2 正则化来解决该问题；
- 计算代价不高，易于理解和实现。

**缺点：**

- 当特征空间很大时，逻辑回归的性能不是很好；
- 容易欠拟合，一般准确度不太高；
- 不能很好地处理大量多类特征或变量；
- 只能处理两分类问题（在此基础上衍生出来的 softmax 可以用于多分类），且必须线性可分；
- 对于非线性特征，需要进行转换。

# 线性回归算法实现

## 打车例子: 

机器学习是从数据中自动分析获得模型，并利用模型对未知数据进行预测。比如举一个栗子，假设有如下打车费用的数据（数据处理或者叫特征工程）：

| 打车距离 | 费用（元） |
| -------- | ---------- |
| 1.0      | 15.0       |
| 2.0      | 18.0       |
| 3.0      | 21.0       |
| 4.0      | 24.0       |
| 5.0      | 27.0       |
| 6.0      | 30.0       |
| 7.0      | 33.0       |
| 8.0      | 36.0       |
| 9.0      | 39.0       |
| 10.0     | 42.0       |
| 11.0     | 45.0       |
| 12.0     | 48.0       |
| 13.0     | 51.0       |
| 14.0     | 54.0       |
| 15.0     | 57.0       |

假设有个乘客坐出租车行驶 20 公里，请问多少钱？作为一个会使用 Python 的普通人，会从数据中学习到规律，这个过程中有人快有人慢，从而定义如下函数，而且定义好函数之后，利用这些对这个函数进行评估，看其是否准确，若是准确，才会去计算 20 公里的打车费。

```python
def calc(distance):
    return distance * 3 + 12
calc(20)
```

而机器学习说直白点就是让其机器学习一个函数（模型），之后再利用这个方法（模型）去计算出（预测）打车多少公里多少钱。

## 让机器如何学习规律

其实本质让机器模仿人类的学习的过程：

先假设，验证：

- 不好，重复假设验证。
- 好，就用规律解决问题。

### 假定

具体我们定义这样的函数：y= w*x+b，假设给 w = 1，b = 1，根据函数计算出打车费用，费用如下：

| 打车距离 | 费用（元） | 计算出的打车费（元） |
| -------- | ---------- | -------------------- |
| 1.0      | 15.0       | 2.0                  |
| 2.0      | 18.0       | 3.0                  |
| 3.0      | 21.0       | 4.0                  |
| 4.0      | 24.0       | 5.0                  |
| 5.0      | 27.0       | 6.0                  |
| 6.0      | 30.0       | 7.0                  |
| 7.0      | 33.0       | 8.0                  |
| 8.0      | 36.0       | 9.0                  |
| 9.0      | 39.0       | 10.0                 |
| 10.0     | 42.0       | 11.0                 |
| 11.0     | 45.0       | 12.0                 |
| 12.0     | 48.0       | 13.0                 |
| 13.0     | 51.0       | 14.0                 |
| 14.0     | 54.0       | 15.0                 |
| 15.0     | 57.0       | 16.0                 |

### 验证

所谓验证，就是让数据中对应公里的打车费与计算出的打车费进行比对，让是否接近，若不接近，那么继续假设，即变更 $w$ 和 $b$ 的值再计算，之后再比对；若接近，停止变更 $w$ 和 $b$ 的值，用这个函数去解决问题。

那这里就引出两个问题：

- 一个怎么判断计算出的打车费接近真实数据的打车费用。
- 一个怎么变更 $w$ 和 $b$ 值。

> 这里就涉及到数学问题，微积分中的求导。

## 成本函数（Cost Function）

成本函数（Cost Function）或损失函数（Loss Function）用于衡量模型的预测结果与实际观测值之间的差异。成本函数是优化算法的基础，通过最小化成本函数，我们可以得到更准确的模型参数。

> 提示，我们可以使用成本函数解决上面的第一个问题。

那这里，我们选用什么函数呢？（下面 z 代表计算出预测值）

$$
C = \frac{1}{n} \sum_{i=1}^n(z_i-y_i)
$$

![差值](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E5%B7%AE%E5%80%BC-1689808656969.png)

$$
C = \frac{1}{n} \sum_{i=1}^n|z_i-y_i|
$$

![绝对值误差](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E7%BB%9D%E5%AF%B9%E5%80%BC%E8%AF%AF%E5%B7%AE-1689808656970.png)

$$
C = \frac{1}{n} \sum_{i=1}^n(z_i-y_i)^2
$$

![均方误差](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE-1689808656970.png)



这里我们使用均方差误差函数，即最后一个。


## 梯度下降（Gradient Descent）

梯度下降（Gradient Descent）是一种常用的优化算法，用于最小化成本函数或损失函数。它是机器学习中最基础的优化算法之一，用于更新模型参数以使成本函数达到最小值。

梯度下降的核心思想是通过计算成本函数关于模型参数的梯度（导数），沿着梯度的反方向更新参数，使成本函数逐步减小，最终收敛到局部最优解或全局最优解。

> 提示，我们可以使用成本函数解决上面的第二个问题。

![二维梯度下降](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D1-1689808656971.png)

![三维梯度下降](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D2-1689808656971.png)

对上面均方误差函数求导如下：

$$
C = \frac{1}{2n} \sum_{i=1}^n(wx_i+b-y_i)^2
$$

$$
\frac{\delta C}{\delta w} = \frac{1}{n} \sum_{i=1}^n(z_i-y_i)x_i
$$

$$
\frac{\delta C}{\delta b} = \frac{1}{n} \sum_{i=1}^n(z_i-y_i)
$$

导数决定了下降方向，而另外一个问题就是下降的步幅（专业说法是学习率 $\eta$），这里我们先可以暂时约定 0.01。

## 代码实现

```python
import numpy as np

def load_data():
  # 数据
  data = np.array([
    [1.0, 15.0],
    [2.0, 18.0],
    [3.0, 21.0],
    [4.0, 24.0],
    [5.0, 27.0],
    [6.0, 30.0],
    [7.0, 33.0],
    [8.0, 36.0],
    [9.0, 39.0],
    [10.0, 42.0],
    [11.0, 45.0],
    [12.0, 48.0],
    [13.0, 51.0],
    [14.0, 54.0],
    [15.0, 57.0]
  ])
  # 划分数据集，分训练集和测试集
  ratio = 0.8
  offset = int(data.shape[0] * ratio)
  train_data = data[:offset]
  test_data =  data[offset:]
  x_train_data = train_data[:, 0:1]
  y_train_data = train_data[:, 1:2]
  x_test_data = test_data[:, 0:1]
  y_test_data = test_data[:, 1:2]
  return x_train_data, y_train_data, x_test_data, y_test_data

class MyNet:
    # 初始 w 和 b
    def __init__(self, w, b):
        self.w = w
        self.b = b
    
    # 计算预测值
    def foward(self, x):
        z = x*self.w + self.b
        return z;

    # 计算成本
    def loss(self, y, z):
      error = z - y
      return np.mean(error * error)

    # 计算梯度
    def gradient(self, x, y, z):
      dw = np.mean((z - y) * x)
      db = np.mean(z - y)
      return dw, db

    # 更新 w 和 b
    def update(self, dw, db, learning_rate):
      self.w = self.w - learning_rate * dw
      self.b = self.b - learning_rate * db

    # 训练
    def train(self, x, y, learning_rate = 0.01):
      for i in range(5000):
        # 计算预测值
        z = self.foward(x)
        # 计算成本
        loss = self.loss(y, z)
        # 计算梯度
        dw, db = self.gradient(x, y, z)
        # 更新参数
        self.update(dw, db, l earning_rate)

        if(i % 100 == 0):
          print('第{}次预测值:'.format(i+1), z)
          print('第{}次成本值:'.format(i+1), loss)
          print('第{}次梯度值:'.format(i+1), dw, db)
          print('第{}次w和b的值:'.format(i+1), self.w, self.b)

# 获取数据
x_train_data, y_train_data, x_test_data, y_test_data = load_data()

# 创建模型对象
myNet = MyNet(1, 1)
# 训练
myNet.train(x_train_data, y_train_data)

# 预测测试数据
z = myNet.foward(x_test_data)
print('测试数据的预测值:', z)
# 评估测试数据
loss = myNet.loss(y_test_data, z)
print('测试数据的成本值:', loss)
```

# 补充内容

## 绘制一个损失值的折线图

需求说明: 记录每次训练训练的损失的图像, 其中x轴代表训练轮次, y轴代表这一轮次的损失值

模板代码:

安装库 `pip install  plotly`

```python
import plotly.graph_objects as go

# 假设您已经有一个包含每次迭代损失值的列表或数组
loss_values = [0.9, 0.85, 0.8, 0.75, 0.73, 0.7, 0.68, 0.65, 0.63, 0.6]  # 示例数据

# 创建迭代次数的列表，长度与损失值列表相同
iterations = list(range(1, len(loss_values) + 1))

# 创建散点图
fig = go.Figure()

fig.add_trace(go.Scatter(
    x=iterations,
    y=loss_values,
    mode='markers+lines',  # 同时绘制散点和折线
    marker=dict(color='blue', size=8),
    line=dict(color='blue', width=2),
    name='损失值'
))

# 添加标题和轴标签
fig.update_layout(
    title='线性模型训练过程中的损失值变化',
    xaxis_title='迭代次数',
    yaxis_title='损失值',
    template='plotly_white',
    xaxis=dict(showgrid=True),
    yaxis=dict(showgrid=True)
)
# 显示图形
fig.show()
```

## 绘制一个梯度下降的函数过程图像

模板代码:

```python
import numpy as np
import plotly.graph_objects as go

# 1. 定义数据集
x = np.array([1, 2, 3, 4, 5])
y_true = np.array([2, 4, 6, 8, 10])

# 2. 定义损失函数和梯度计算
def mse_loss(w, b, x, y_true):
    y_pred = w * x + b
    loss = np.mean((y_true - y_pred) ** 2)
    return loss

def compute_gradients(w, b, x, y_true):
    N = len(x)
    y_pred = w * x + b
    dw = (-2 / N) * np.sum(x * (y_true - y_pred))
    db = (-2 / N) * np.sum(y_true - y_pred)
    return dw, db

# 3. 实现梯度下降算法
w = 0.0  # 初始化权重
b = 0.0  # 初始化偏置
learning_rate = 0.01
num_iterations = 100

# 记录梯度下降的路径
w_history = []
b_history = []
loss_history = []

for _ in range(num_iterations):
    loss = mse_loss(w, b, x, y_true)
    dw, db = compute_gradients(w, b, x, y_true)
    w -= learning_rate * dw
    b -= learning_rate * db
    w_history.append(w)
    b_history.append(b)
    loss_history.append(loss)

# 4. 绘制梯度下降路径的三维图形
gd_path = go.Scatter3d(
    x=w_history,
    y=b_history,
    z=loss_history,
    mode='lines+markers',
    line=dict(color='blue', width=4),
    marker=dict(size=5, color='red'),
    name='梯度下降路径'
)

# 创建图形对象
fig = go.Figure(data=[gd_path])

# 更新布局
fig.update_layout(
    title='梯度下降过程中的参数变化',
    scene=dict(
        xaxis_title='权重 w',
        yaxis_title='偏置 b',
        zaxis_title='损失 Loss'
    ),
    width=800,
    height=600
)

# 显示图形
fig.show()
```

## 绘制一个损失函数图像

需求: 损失函数是均方误差:
$$
C = \frac{1}{n} \sum_{i=1}^n(z_i-y_i)^2
$$
其中$z_i$是预测值: $z_i=w*x_i+b$

> ![image-20241015184946971](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/image-20241015184946971.png)

代码实现

```python
import numpy as np
import plotly.graph_objects as go

# 1 加载数据 训练数据(x_train_data, y_train_data) 测试数据(x_test_data, y_test_data)
x_train_data,y_train_data, x_test_data, y_test_data = load()
x = x_train_data
y_true = y_train_data

# 2. 定义损失函数
def mse_loss(w, b, x, y_true):
    y_pred = w * x + b
    loss = np.mean((y_true - y_pred) ** 2)
    return loss

# 3. 创建参数网格
w_values = np.linspace(0, 24, 500)   # w 的取值范围
b_values = np.linspace(0, 24, 500)   # b 的取值范围
W, B = np.meshgrid(w_values, b_values)

# 4. 计算损失值
Loss = np.zeros_like(W)
for i in range(W.shape[0]):
    for j in range(W.shape[1]):
        Loss[i, j] = mse_loss(W[i, j], B[i, j], x, y_true)

# 5. 绘制损失函数图像
fig = go.Figure(data=[go.Surface(z=Loss, x=W, y=B)])
fig.update_layout(title='均方误差（MSE）损失函数',
                  scene=dict(
                      xaxis_title='权重 w',
                      yaxis_title='偏置 b',
                      zaxis_title='损失 Loss'
                  ),
                  autosize=False,
                  width=800,
                  height=800)
fig.show()
```

## 完整的代码实现

```python
# 导入基本模块
import numpy as np
import plotly.graph_objects as go

# 加载数据的函数load
# 1 加载数据 训练数据(x_train_data, y_train_data) 测试数据(x_test_data, y_test_data)
def load(ratio=0.8):
    data = np.array([
        [1.0, 15.0],
        [2.0, 18.0],
        [3.0, 21.0],
        [4.0, 24.0],
        [5.0, 27.0],
        [6.0, 30.0],
        [7.0, 33.0],
        [8.0, 36.0],
        [9.0, 39.0],
        [10.0, 42.0],
        [11.0, 45.0],
        [12.0, 48.0],
        [13.0, 51.0],
        [14.0, 54.0],
        [15.0, 57.0]
    ])
    offset = int(data.shape[0]*0.8)
    train_data=data[:offset]
    test_data = data[offset:]
    x_train_data = train_data[:,:1]
    y_train_data = train_data[:,1:]
    x_test_data = test_data[:,:1]
    y_test_data = test_data[:,1:]
    return x_train_data,y_train_data, x_test_data, y_test_data

# 定义一个网络模型类 MyNet
class MyNet:
    # 初始化函数
    def __init__(self, w=0, b=0) -> None:
        self.w = w
        self.b = b
        self.loss_values=[]
        self.loss_info_list=[]

    # 3 定义模型, 计算预测结果 y = w*x + b
    def predict(self, x):
        y = self.w * x + self.b 
        return y
    
    # 4 进行评价, 定义一个损失函数
    def loss(self, y, z):
        error = z-y
        return np.mean(error*error)
    
    # 5 计算梯度
    def gradient(self,x,y,z):
        dw = np.mean((z-y)*x)
        db = np.mean(z-y)
        return dw,db
    
    # 6 更新梯度
    def update(self, dw,db,learning_rate=0.01):
        self.w = self.w - learning_rate*dw
        self.b = self.b - learning_rate*db

    # 1 定义一个模型训练的方法 train
    def train(self,x_train_data,y_train_data,epoch=2000,learning_rate=0.01):
        for i in range(epoch):
            # 预测结果
            y_predict = self.predict(x_train_data)
            # 计算损失
            loss_value = self.loss(y_train_data,y_predict)
            # 计算梯度
            dw,db = self.gradient(x_train_data,y_train_data,y_predict)
            # 记录对应的loss, w, b 的对应数据
            self.loss_info_list.append((self.w,self.b,loss_value))
            # 更新梯度
            self.update(dw,db,learning_rate)
            # 把所有的损失都记录起来
            self.loss_values.append(loss_value)
            print(f'训练轮次:{i+1}, 当前的损失:{loss_value}, 更新后的w:{self.w}, b:{self.b}')

# 测试代码
if __name__=="__main__":
    x_train_data,y_train_data, x_test_data, y_test_data = load()
    # 创建一个实例对象
    model = MyNet(w=0,b=0)
    model.train(x_train_data,y_train_data,epoch=5000)

    # 1 绘制一个损失值的折线图
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=list(range(1,len(model.loss_values)+1)),
        y=model.loss_values,
        mode='markers+lines',  # 同时绘制散点和折线
        marker=dict(color='blue', size=8),
        line=dict(color='blue', width=2),
        name='损失值'
    ))

    # 添加标题和轴标签
    fig.update_layout(
        title='线性模型训练过程中的损失值变化',
        xaxis_title='迭代次数',
        yaxis_title='损失值',
        template='plotly_white',
        xaxis=dict(showgrid=True),
        yaxis=dict(showgrid=True)
    )
    # 显示图形
    # fig.show()

    # 2 绘制一个梯度下降的函数过程图像
    gd_path = go.Scatter3d(
        x=[value[0] for value in model.loss_info_list],
        y=[value[1] for value in model.loss_info_list],
        z=[value[2] for value in model.loss_info_list],
        mode='lines+markers',
        line=dict(color='blue', width=4),
        marker=dict(size=5, color='red'),
        name='梯度下降路径'
    )

    # 创建图形对象
    fig = go.Figure(data=[gd_path])

    # 更新布局
    fig.update_layout(
        title='梯度下降过程中的参数变化',
        scene=dict(
            xaxis_title='权重 w',
            yaxis_title='偏置 b',
            zaxis_title='损失 Loss'
        ),
        width=800,
        height=600
    )

    # 显示图形
    # fig.show()

    # 3 函数损失图结合梯度下降图
    # 2. 定义损失函数
    def mse_loss(w, b, x, y_true):
        y_pred = w * x + b
        loss = np.mean((y_true - y_pred) ** 2)
        return loss
    # 打印w, b的取值范围
    w_min=np.min([value[0] for value in model.loss_info_list])
    w_max=np.max([value[0] for value in model.loss_info_list])
    b_min=np.min([value[1] for value in model.loss_info_list])
    b_max=np.max([value[1] for value in model.loss_info_list])
    # 3. 创建参数网格
    w_values = np.linspace(w_min-1, w_max+1, 500)   # w 的取值范围
    b_values = np.linspace(b_min-1, b_max+1, 500)   # b 的取值范围
    W, B = np.meshgrid(w_values, b_values)
    # 4. 计算损失值
    Loss = np.zeros_like(W)
    for i in range(W.shape[0]):
        for j in range(W.shape[1]):
            Loss[i, j] = mse_loss(W[i, j], B[i, j], x_train_data, y_train_data)
    surface = go.Surface(
        x=W,
        y=B,
        z=Loss,
        colorscale='Viridis',
        opacity=0.8,
        name='损失函数曲面'
    )

    # 创建梯度下降路径的线条
    gd_path = go.Scatter3d(
        x=[value[0] for value in model.loss_info_list],
        y=[value[1] for value in model.loss_info_list],
        z=[value[2] for value in model.loss_info_list],
        mode='lines+markers',
        line=dict(color='red', width=4),
        marker=dict(size=5, color='red'),
        name='梯度下降路径'
    )

    # 创建图形对象
    fig = go.Figure(data=[surface, gd_path])

    # 更新布局
    fig.update_layout(
        title='梯度下降过程中的损失函数变化',
        scene=dict(
            xaxis_title='权重 w',
            yaxis_title='偏置 b',
            zaxis_title='损失 Loss'
        ),
        width=800,
        height=600
    )
    # 显示图形
    fig.show()
```



# 作业

## 作业1: 完成上面的用户打车案例代码实现

## 作业2: 使用matplotlib绘制下面的损失图形

其中x轴代表训练轮次

y轴代表训练的一个损失值

> ![image-20241008191304762](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/image-20241008191304762.png)

## 作业3: 使用plotly绘制梯度下降图形

> ![image-20241008191432353](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/image-20241008191432353.png)