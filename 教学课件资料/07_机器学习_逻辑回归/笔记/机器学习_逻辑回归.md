# 特征工程

## 重要性

大家都听过美国计算机科学家 Peter Norvig（彼得 诺维格） 的 $2$ 句经典名言：

> 基于大量数据的简单模型优于基于少量数据的复杂模型。

这句说明了数据量的重要性。

> 更多的数据优于聪明的算法，而好的数据优于多的数据。

这句则是说的特征工程的重要性。

所以，如何基于给定数据来发挥更大的数据价值就是特征工程要做的事情。

在 $16$ 年的一项调查中发现，数据科学家的工作中，有大约 $80\%$ 的时间都在获取、清洗和组织数据。其它的时间不到 $20\%$。详情如下：

- 收集数据集：$19\%$
- 清洗和组织数据：$60\%$
- 设置训练集：$3\%$
- 挖掘数据模式：$9\%$
- 调整算法：$5\%$
- 其他：$4\%$

特征工程是机器学习流程里最花时间的工作，也是最重要的工作内容之一。

## 定义

我们先来看看特征工程在机器学习流程中的位置：

<img src="https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B5%81%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE.png" alt="特征工程在机器学习流程中的位置" style="zoom:40%;" />

从上图可以看出，特征工程处在原始数据和特征之间。他的任务就是将原始数据「翻译」成特征的过程。

特征：是原始数据的数值表达方式，是机器学习算法模型可以直接使用的表达方式。

特征工程是一个过程，这个过程将数据转换为能更好的表示业务逻辑的特征，从而提高机器学习的性能。

这么说可能不太好理解。其实特征工程跟做饭很像：我们将食材购买回来，经过清洗、切菜，然后开始根据自己的喜好进行烹饪，做出美味的饭菜。

<img src="https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B8%8E%E7%83%B9%E9%A5%AA.png" alt="特征工程跟做饭很像" style="zoom:40%;" />

上面的例子中：

- 食材就好像原始数据

- 清洗、切菜、烹饪的过程就好像特征工程

- 最后做出来的美味饭菜就是特征

人类是需要吃加工过的食物才行，这样更安全也更美味。机器算法模型也是类似，原始数据不能直接喂给模型，也需要对数据进行清洗、组织、转换。最后才能得到模型可以消化的特征。

除了将原始数据转化为特征之外，还有 $2$ 个容易被忽视的重点：

- 更好的表示业务逻辑
  - 特征工程可以说是业务逻辑的一种数学表达。
  - 我们使用机器学习的目的是为了解决业务中的特定问题。相同的原始数据有很多种转换为特征的方式，我们需要选择那些能够「更好的表示业务逻辑」，从而更好的解决问题。
- 提高机器学习性能
  - 性能意味着更短时间和更低成本，哪怕相同的模型，也会因为特征工程的不同而性能不同。所以我们需要选择那些可以发挥更好性能的特征工程。

## 探索性数据分析（EDA）

探索性数据分析（Exploratory Data Analysis）是拿到原始数据后，通过技术手段帮助自己更好的理解数据、提取出「好特征」的过程。

探索性数据分析的过程大致分为 $3$ 步：

- 数据分类

- 数据可视化

- 洞察数据

### 数据分类

当我们拿到数据后，第一步就是把这些数据进行分类，然后用不同方法来处理不同类型的数据。

数据由粗到细可以按照下面的方式来分类：

<img src="https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E6%95%B0%E6%8D%AE%E7%BB%86%E5%88%86%E6%96%B9%E6%B3%95.png" alt="数据细分方法" style="zoom:40%;" />

**结构化数据 VS 非结构化数据**

| 类别         | 描述                                     | 示例                           |
| ------------ | ---------------------------------------- | ------------------------------ |
| 结构化数据   | 能够用表格来组织的数据都算是结构化的数据 | Excel 里的数据、MySQL 里的数据 |
| 非结构化数据 | 非表格形式组织的都是                     | 文本、语音、图片、视频         |

**定量数据 VS 定性数据**

| 类别     | 描述                         | 示例       |
| -------- | ---------------------------- | ---------- |
| 定量数据 | 数值类型，衡量某样东西的数量 | 商品价格   |
| 定性数据 | 类别，描述某样东西的性质     | 男人，女人 |

**数据的 4 个等级**

| 等级                       | 描述                                                         | 示例                                            |
| -------------------------- | ------------------------------------------------------------ | ----------------------------------------------- |
| 定类等级（norminal level） | 是数据的第一个等级，其结构最弱，只需要按照名称来分类         | 血型（A，B，AB，O）、货币（美元，人民币）、颜色 |
| 定序等级（ordinal level）  | 定序等级在定类等级的基础上加了自然排序，这样我们就可以对不同数据进行比较 | 餐厅的评星，公司的考核等级                      |
| 定距等级（interval level） | 定距等级一定是数值类型的，并且这些数值不仅可以用来排序，还可以用来加减 | 华氏度、摄氏度                                  |
| 定比等级（ratio level）    | 在定距等级的基础上，不但可以做加减的运算，还可以做乘除的运算 | 金钱、重量                                      |





### 数据可视化

为了更好的洞察数据，我们可以将数据可视化，从而更好的观察数据的特点。

常用的数据可视化有下面几种：

<img src="https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt="数据可视化" style="zoom:40%;" />

上面的 4 个数据等级需要对应不同的可视化方法，下面整理了一个表格，可以帮助大家更好的选择可视化的方案。

下面是一些基础的可视化方案，在实际应用中，会有更复杂的，组合图表可以使用。

| 数据等级 | 属性           | 描述性统计                       | 图表                         |
| :------- | :------------- | :------------------------------- | :--------------------------- |
| 定类     | 离散、无序     | 频率占比、众数                   | 条形图、饼图                 |
| 定序     | 有序类别、比较 | 频率、众数、中位数、百分位数     | 条形图、饼图                 |
| 定距     | 数字差别有意义 | 频率、众数、中位数、均值、标准差 | 条形图、饼图、箱线图         |
| 定比     | 连续           | 均值、标准差                     | 条形图、曲线图、饼图、箱线图 |

### 洞察数据

数据的可视化可以帮助我们更好的洞察数据，我们可以更高效的发现哪些数据更重要，不同数据之间可能存在的关系，哪些数据会相互影响…

## 分类特征

> 分类特征是最常见的一种特征类型，需要做一些处理之后才可以喂给算法。
> 本文介绍了种常见的处理方式：序列编码、独热编码、散列编码等等。

### 什么是分类特征

分类特征是用来表示分类的，他不像数值类特征是连续的，分类特征是离散的。

比如：性别，城市，颜色，IP 地址，用户的账号 ID

<img src="https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E5%88%86%E7%B1%BB%E7%89%B9%E5%BE%81.png" alt="分类特征" style="zoom:40%;" />

有些分类特征也是数值，比如账号 ID，IP 地址。但是这些数值并不是连续的。

连续的数字是数值类特征，离散的数字是分类特征。

### 小型分类特征的编码方式

#### 自然数编码/序列编码-Ordinal Encoding

某些分类本来就有一定的排序，这种情况下就可以使用简单的自然数编码。

例如学位：

| 学位 | 编码 |
| ---- | ---- |
| 学士 | 0    |
| 硕士 | 1    |
| 博士 | 2    |

| 性别 | 编码 |
| ---- | ---- |
| 男   | 1    |
| 女   | 0    |

#### 独热编码-One-Hot Encoding

对于城市、颜色、品牌、材质…这些特征就不适合用自然数编码，因为这些特征是没有排序关系的。

使用独热编码可以让不同的分类处在「平等的地位」，不会因为数值的大小而对分类造成影响。

例如颜色分类（假设只有 $3$ 种颜色）：

原始数据:

| 编号 | 颜色 |
| ---- | ---- |
| 1    | 红色 |
| 2    | 黄色 |
| 3    | 蓝色 |

编码后的数据:
| 编号 | 红色 | 黄色 | 蓝色 |
| ---- | ---- | ---- | ---- |
| 1    | 1    | 0    | 0    |
| 2    | 0    | 1    | 0    |
| 3    | 0    | 0    | 1    |

原始数据

| 编号 | 血型 |
| ---- | ---- |
| 1    | O    |
| 2    | A    |
| 3    | B    |
| 4    | AB   |

编码后的数据:
| 编号 | O    | A    | B    | AB   |
| ---- | ---- | ---- | ---- | ---- |
| 1    | 1    | 0    | 0    | 0    |
| 2    | 0    | 1    | 0    | 0    |
| 3    | 0    | 0    | 1    | 0    |
| 4    | 0    | 0    | 0    | 1    |

跟独热编码类似还有「虚拟编码-Dummy Encoding」和「效果编码-Effect Encoding」。

实现方式比较相似，不过有一些略微的差别，并且适用在不同的场景。

### 大型分类特征的编码方式

#### 目标编码-Target Encoding

目标编码是表示分类列的一种非常有效的方法，并且仅占用一个特征空间，也称为均值编码。该列中的每个值都被该类别的平均目标值替代。这可以更直接地表示分类变量和目标变量之间的关系。

假设你在做一个预测房屋价格的模型，其中一个特征是房屋所在的城市。你有四个城市：A、B、C、D，目标变量是房屋价格。

| 城市 | 房屋价格 |
| ---- | -------- |
| A    | 100,000  |
| A    | 120,000  |
| B    | 80,000   |
| B    | 90,000   |
| C    | 150,000  |
| D    | 130,000  |

对城市特征进行目标编码意味着计算每个城市房屋价格的平均值，并用这个平均值来替换城市名称：

| 城市 | 编码后的值 |
| ---- | ---------- |
| A    | 110,000    |
| B    | 85,000     |
| C    | 150,000    |
| D    | 130,000    |

#### 散列编码-Hash Encoding

散列函数也是大家常听到的哈希函数。散列函数是一个确定性函数，它映射一个潜在的无界整数到有限整数范围 $[1，m]$。

假如有一个分类有 $1$ 万个值，如果使用独热编码，编码会非常长。而使用了散列编码，不管分类有多少不同的值，都会转换成长度固定的编码。

假设你有一个特征是网页 URL，由于 URL 的数量可能非常庞大，直接使用它们作为特征可能不太可行。通过哈希编码，你可以将每个 URL 映射到一个长度为 $4$ 的向量：

| URL                                                    | 哈希编码后的向量 |
| ------------------------------------------------------ | ---------------- |
| [www.example.com](http://www.example.com/)             | [2, 0, 1, 0]     |
| [www.another.com](http://www.another.com/)             | [0, 1, 0, 2]     |
| [www.somethingelse.com](http://www.somethingelse.com/) | [1, 1, 1, 0]     |

在这个例子中，我们使用了一个简化的哈希函数，实际的哈希函数会根据具体的算法将 URL 映射到固定长度的向量。

#### 分箱计数-Bin-Counting

分箱计数的思维有点复杂：他不是用分类变量的值作为特征，而是使用目标变量取这个值的条件概率。换句话说，我们不对分类变量的值进行编码，而是要计算分类变量值与要预测的目标变量之间的相关统计量。

假设我们有一个数据集，目标是预测客户是否会购买某个产品（是或否），并且我们有一个高基数的分类特征，如客户的职业。

- 职业：软件工程师、医生、教师等。
- 目标变量：是否购买（$1$ 表示购买，$0$ 表示不购买）。

分箱计数的步骤：

- 对于每个职业类别，计算给定职业的客户中购买产品的条件概率。
- 使用这个条件概率作为编码值替换原始的职业类别。

假设我们有以下数据：

| id   | 职业       | 是否购买 |
| ---- | ---------- | -------- |
| 111  | 软件工程师 | 是       |
| 222  | 软件工程师 | 否       |
| 333  | 教师       | 是       |
| 444  | 医生       | 否       |

| 职业       | 购买 | 不购买 | 购买产品的条件概率   |
| ---------- | ---- | ------ | -------------------- |
| 软件工程师 | 80   | 20     | 80 / (80 + 20) = 0.8 |
| 医生       | 30   | 70     | 30 / (30 + 70) = 0.3 |
| 教师       | 50   | 50     | 50 / (50 + 50) = 0.5 |

| id   | 职业 | 是否购买 |
| ---- | ---- | -------- |
| 111  | 0.8  | 1        |
| 222  | 0.8  | 0        |
| 333  | 0.5  | 1        |
| 444  | 0.3  | 0        |

## 数值特征

> 数值类特征是最常见的一种特征类型，数值可以直接喂给算法。
> 为了提升效果，我们需要对数值特征做一些处理，本文介绍了 $4$ 种常见的处理方式：缺失值处理、二值化、分桶、缩放。

### 什么是数值类特征

数值类特征就是可以被实际测量的特征。例如：

- 人的身高、体重、三维。
- 商品的访问次数、加入购物车次数、最终销量。
- 登录用户中有多少新增用户、回访用户。

> **数值类的特征可以直接喂给算法，为什么还要处理？**
>
> 因为好的数值特征不仅能表示出数据隐藏的中的信息，而且还与模型的假设一致。通过合适的数值变换就可以带来很好的效果提升。
>
> 例如线性回归、逻辑回归对于数值的大小很敏感，所以需要进行缩放。

<img src="https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E6%95%B0%E5%80%BC%E7%89%B9%E5%BE%81%E5%85%B3%E6%B3%A8%E7%82%B9.png" alt="数值特征关注点" style="zoom:40%;" />

对于数值类特征，我们主要关注 $2$ 个点：大小和分布。下面提到的 $4$ 种处理方式都是围绕大小和分布来优化的。

### 数值类特征常用的 4 种处理方式

#### 缺失值处理

在实际问题中，经常会遇到数据缺失的情况。缺失值对效果会产生较大的影响。所以需要根据实际情况来处理。

对于缺失值常用 $3$ 种处理方式：

1. 填充缺失值（均值、中位数、模型预测…）
2. 删除带有缺失值的行
3. 直接忽略，将缺失值作为特征的一部分喂给模型进行学习

#### 二值化

这种处理方式通常用在计数的场景，例如：访问量、歌曲的收听次数…

举例：根据用户的听音乐的数据来预测哪些歌曲更受欢迎。

假设大部分人听歌都很平均，会不停的听新的歌曲，但是有一个用户 $24$ 小时的不停播放同一首歌曲，并且这个歌曲很偏门，导致这首歌的总收听次数特别高。如果用总收听次数来喂给模型，就会误导模型。这时候就需要使用「二值化」。

| 用户名 | 歌曲     | 听的次数 | 二值化处理 |
| ------ | -------- | -------- | ---------- |
| 张三   | 一生所爱 | 96       | 1          |
| 李四   | 一生所爱 | 10       | 1          |
| 王五   | 一生所爱 | 0        | 0          |

同一个用户，把同一首歌听了 $N$ 遍，只计数 $1$，这样就能找出大家都喜欢的歌曲来推荐。

#### 分桶/分箱

拿每个人的收入举例，大部分人的收入都不高，极少数人的收入极其高，分布很不均匀。有些人月收入 $3000$，有些人月收入 $30$ 万，跨了好几个数量级。

这种特征对于模型很不友好。这种情况就可以使用分桶来处理。分桶就是将数值特征分成不同的区间，将每个区间看做一个整体。

常见的分桶：

1. 年龄分布
2. 商品价格分布
3. 收入分布

常用的分桶方式：

1. 固定数值的分桶（例如年龄分布：0-12 岁、13-17 岁、18-24 岁…）
2. 分位数分桶（例如淘宝推荐的价格区间：$30\%$ 用户选择最便宜的价格区间、$60\%$ 用户选择的中等价格区间、$9\%$ 的用户选择最贵的价格区间）
3. 使用模型找到最佳分桶

<img src="https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E5%88%86%E6%A1%B6.png" alt="分桶" style="zoom:40%;" />

假设我们同样有一组人的年龄数据，我们想要将这些数据分成几个年龄段，我们可以将年龄分为三个桶：

- $18$ 到 $29$ 岁
- $30$ 到 $39$ 岁
- $40$ 岁及以上

分桶处理后的数据：

- $22$, $19$, $27$ -> 第一个桶，用 $0$ 表示
- $34$, $33$ -> 第二个桶，用 $1$ 表示
- $45$ -> 第三个桶，用 $2$ 表示

| 年龄 | 分桶处理 |
| ---- | -------- |
| 22   | 0        |
| 34   | 1        |
| 19   | 0        |
| 45   | 2        |
| 27   | 0        |
| 33   | 1        |
| 22   | 0        |
| 34   | 1        |
| 19   | 0        |
| 45   | 2        |
| 27   | 0        |
| 33   | 1        |

#### 缩放

线性回归、逻辑回归对于数值的大小很敏感、不同特征尺度相差很大的话会严重影响效果。所以需要将不同量级的数值进行归一化。将不同的数量级缩放到同一个静态范围中（例如：$0$~$1$，$-1$~$1$）。

常用缩放方式：

- min-max 标准化

  又称为极差法，它是将数据集中某一列数值缩放到 $0$ 和 $1$ 之间。 它的计算方法是：


$$
x = \frac{x-x_{min}}{x_{max}-x_{min}}
$$

  该是对原始数据的线性变换。min-max 标准化方法保留了原始数据之间的相互关系，但是如果标准化后，新输入的数据超过了原始数据的取值范围，即不在原始区间 $[x_{min}, x_{max}]$ 中，则会产生越界错误。因此这种方法适用于原始数据的取值范围已经确定的情况。

  

- 均值归一化

  与 min-max 标准化类似，区别是使用平均值 $\mu$ 计算，公式如下：
  $$
  x = \frac{x-\mu}{x_{max}-x_{min}}
  $$

  该方法把数据调到 $[-1, 1]$，平均值为 $0$。适合一些假设数据中心为 $0$（zero centric data）的算法，比如主成分分析（PCA）。
  
- z-score 标准化

  又称标准差标准化，代表的是分值偏离均值的程度，经过处理的数据符合标准正态分布，即均值为 $0$，标准差为 $1$。其转化函数为（其中 $\mu$ 为所有数据的均值，$\sigma$ 为所有数据的标准差。）：
  $$
  x = \frac{x-\mu}{\sigma}
  $$

该方法假设数据是正态分布，但这个要求并不十分严格，如果数据是正态分布则该技术会更有效。当我们使用的算法假设数据是正态分布时，可以使用 Standardization，比如线性回归、逻辑回归、线性判别分析。因为 Standardization 使数据平均值为 $0$ ，也可以在一些假设数据中心为 $0$（zero centric data）的算法中使用，比如主成分分析（PCA）。

## 练习

下面是某球员的比赛主要数据及参加比赛的结果，希望通过球员数据预测比赛结果，但现在我们首先要对数据进行特征工程。

| 得分 | 助攻 | 篮板 | 比赛结果 |
| ---- | ---- | ---- | -------- |
| 23   | 12   | 7    | 胜       |
| 22   | 10   | 9    | 败       |
| 26   | 9    | 12   | 败       |
| 30   | 9    | 9    | 胜       |
| 25   | 3    | 9    | 败       |
| 27   | 6    | 9    | 胜       |
| 21   | 8    | 8    | 胜       |
| 23   | 3    | 7    | 败       |
| 22   | 4    | 11   | 胜       |
| 22   | 6    | 5    | 胜       |

> 数据集（data set）：指的是下面所有数据，训练集（training set），表示用用来训练模型的数据；测试集（test set），表示用来测试模式的数据。
>
> 特征（feature，又叫输入，input）：拿上面数据来说，指的是得分，助攻，篮板，而特征值（feature value）指的是这些特征对应的值。
>
> 标签（label，又叫输出，output）：拿上面数据来说，指定的比赛结果。

```python
import numpy as np

data = np.array([
    [23, 12, 7, '胜'],
    [22, 10, 9, '败'],
    [26, 9, 12, '败'],
    [30, 9, 9, '胜'],
    [25, 3, 9, '败'],
    [27, 6, 9, '胜'],
    [21, 8, 8, '胜'],
    [23, 3, 7, '败'],
    [22, 4, 11, '胜'],
    [22, 6, 5, '胜']
])

# 序列编码
data[data=='胜'] = 1
data[data=='败'] = 0

# min-max 标准化
data = data.astype('float32')
min_vals = data.min(axis=0)
max_vals = data.max(axis=0)

for i in range(3):
    data[:,i] = (data[:,i]  - min_vals[i]) / (max_vals[i] - min_vals[i])

print(data)
```

# Scikit-learn

## 介绍 

Scikit-learn（曾叫做 scikits.learn 还叫做 sklearn）是用于 Python 编程语言的自由软件机器学习库。它的特征是具有各种分类、回归和聚类算法，包括支持向量机、随机森林、梯度提升、K-means 聚类，并且可以与 Python 数值库 NumPy 及科学库 SciPy 进行互操作。

在 Sklearn 里面有六大任务模块：分别是分类（Classification）、回归（Regression）、聚类（Clustering）、降维（Dimensionality Reduction）、模型选择（Model Selection）和预处理（Preprocessing），如下图：

![sklearn](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/sklearn.png)



要使用上述六大模块的方法，可以用以下的伪代码，注意 import 后面我用的都是一些通用名称，如 XxxClassifier，XxxRegressor，XxxModel，具体化的名称由具体问题而定。比如：RandomForestClassifier 是随机森林分类器、LinearRegression 是线性回归器、KMeans 是 K 均值聚类模型。

## 安装

通过如下命令可以安装 Sklearn：

```
pip install scikit-learn==1.0.2
```

## 内置数据集

在 Sklean 里，模型能即用的数据有两种形式：

- Numpy 二维数组 (ndarray) 的稠密数据 (dense data)，通常都是这种格式。
- SciPy 矩阵 (scipy.sparse.matrix) 的稀疏数据 (sparse data)，比如文本分析每个单词 (字典有 $100000$ 个词) 做独热编码得到矩阵有很多 $0$，这时用 ndarray 就不合适了，太耗内存。

Sklearn 内置了一些机器学习的数据集，其中包括 iris（鸢尾花）数据集、乳腺癌数据集、波士顿房价数据集、糖尿病数据集、手写数字数据集、体能训练数据集和酒质量数据集。

### 波士顿房价数据集

Scikit-learn 自带波士顿房价数据集，该数据集来源于 $1978$ 年美国某经济学杂志上。该数据集包含若干波士顿房屋的价格及其各项数据，每个数据项包含 $14$ 个相关特征数据，分别是房屋均价及周边犯罪率、是否在河边、师生比等相关信息，其中最后一项数据是该区域房屋均价。

波士顿房价数据集是一个回归问题，共有 $506$ 个样本，$13$ 个输入变量和 $1$ 个输出变量。

**数据集样本实例数：**$506$ 个。

**特征个数：**$13$ 个特征属性和 $1$ 个目标数值。

**特征信息：**

1. CRIM - 城镇人均犯罪率
2. ZN - 住宅用地所占比例（每 $25000$ 平方英尺）
3. INDUS - 城镇非商业用地所占比例
4. CHAS - 查尔斯河的指标虚拟化（区域在河附近用 $1$ 表示，否则为 $0$）
5. NOX - 一氧化氮浓度
6. RM - 每栋住宅的房间数
7. AGE - $1940$ 年之前建成的自用住宅的比例
8. DIS - 距离 $5$ 个波士顿就业中心的加权距离
9. RAD - 距离高速公路的便利指数
10. TAX - 每 $10000$ 美元的全值财产税率
11. PTRATIO - 城镇师生比例
12. B - 城镇中黑人比例
13. LSTAT - 低收入人群的百分比
14. MEDV - 房屋房价的中位数（以千美元为单位）

```python
from sklearn.datasets import load_boston

# 加载波士顿房价数据集
boston = load_boston()
X = boston.data
y = boston.target
```

### Iris（鸢尾花）数据集

Iris 数据集是常用的分类实验数据集，它首次出现在著名的英国统计学家和生物学家 Ronald Fisher $1936$ 年的论文《The use of multiple measurements in taxonomic problems》中，被用来介绍线性判别式分析。Iris 也称鸢尾花卉数据集，是一类多重变量分析的数据集。数据集包含 $150$ 个数据集，分为 $3$ 类，每类 $50$ 个数据，每个数据包含 $4$ 个属性。可通过花萼长度，花萼宽度，花瓣长度，花瓣宽度 4 个属性预测鸢尾花卉属于（Setosa，Versicolour，Virginica）三个种类中的哪一类。

**数据集样本实例数：**$150$（每个类都有 $50$ 个样本实例）。

**特征个数：**$4$ 个数字特征和 $1$ 个类别特征。

**特征信息：**$4$ 个特征属性分别是 sepal length（花萼长度）、sepal width（花萼宽度）、petal length（花瓣长度）、petal width（花瓣宽度），单位是 cm（厘米）。

**数据集相关统计摘要：**主要是数据集中特征相关数据的统计数据，具体如下：

|              | Min  | Max  | Mean | SD（标准差） | Class Correlation |
| ------------ | ---- | ---- | ---- | ------------ | ----------------- |
| sepal length | 4.3  | 7.9  | 5.84 | 0.83         | 0.7826            |
| sepal width  | 2.0  | 4.4  | 3.05 | 0.43         | -0.4194           |
| petal length | 1.0  | 6.9  | 3.76 | 1.76         | 0.9490            |
| petal width  | 0.1  | 2.5  | 1.20 | 0.76         | 0.9565            |

```python
from sklearn.datasets import load_iris

# 加载sklearn自带的iris（鸢尾花）数据集
dataset = load_iris()

# 提取特征数据和目标数据
X = dataset.data
y = dataset.target
```

### 乳腺癌数据集

Scikit-learn 内置的乳腺癌数据集来自加州大学欧文分校机器学习仓库中的威斯康辛州乳腺癌数据集。

乳腺癌数据集是一个共有 $569$ 个样本、$30$ 个输入变量和 $2$ 个分类的数据集。

**数据集样本实例数：**$569$ 个。

**特征个数：**$30$ 个特征属性和 $2$ 个分类目标（恶性 - Malignant，良性 - Benign）。

**特征信息：**

$30$ 个数值型测量结果由数字化细胞核的 $10$ 个不同特征的均值、标准差和最差值（即最大值）构成。这些特征包括：

| 序号 | 属性                                                   | 最小值 | 最大值 |
| ---- | ------------------------------------------------------ | ------ | ------ |
| 1    | radius(mean) - 半径（平均值）                          | 6.981  | 28.11  |
| 2    | texture(mean) - 质地（平均值）                         | 9.71   | 39.28  |
| 3    | perimeter(mean) - 周长（平均值）                       | 43.79  | 188.5  |
| 4    | area(mean) - 面积（平均值）                            | 143.5  | 2501.0 |
| 5    | smoothness(mean) - 光滑度（平均值）                    | 0.053  | 0.163  |
| 6    | compactness(mean) - 致密性（平均值）                   | 0.019  | 0.345  |
| 7    | concavity(mean) - 凹度（平均值）                       | 0.0    | 0.427  |
| 8    | concave points(mean) - 凹点（平均值）                  | 0.0    | 0.201  |
| 9    | symmetry(mean) - 对称性（平均值）                      | 0.106  | 0.304  |
| 10   | fractal dimension(mean) - 分形维数（平均值）           | 0.05   | 0.097  |
| 11   | radius(standard error) - 半径（标准差）                | 0.112  | 2.873  |
| 12   | texture(standard error) - 质地（标准差）               | 0.36   | 4.885  |
| 13   | perimeter(standard error) - 周长（标准差）             | 0.757  | 21.98  |
| 14   | area(standard error) - 面积（标准差）                  | 6.802  | 542.2  |
| 15   | smoothness(standard error) - 光滑度（标准差）          | 0.002  | 0.031  |
| 16   | compactness(standard error) - 致密性（标准差）         | 0.002  | 0.135  |
| 17   | concavity(standard error) - 凹度（标准差）             | 0.0    | 0.396  |
| 18   | concave points(standard error) - 凹点（标准差）        | 0.0    | 0.053  |
| 19   | symmetry(standard error) - 对称性（标准差）            | 0.008  | 0.079  |
| 20   | fractal dimension(standard error) - 分形维数（标准差） | 0.001  | 0.03   |
| 21   | radius(worst) - 半径（最大值）                         | 7.93   | 36.04  |
| 22   | texture(worst) - 质地（最大值）                        | 12.02  | 49.54  |
| 23   | perimeter(worst) - 周长（最大值）                      | 50.41  | 251.2  |
| 24   | area(worst) - 面积（最大值）                           | 185.2  | 4254.0 |
| 25   | smoothness(worst) - 光滑度（最大值）                   | 0.071  | 0.223  |
| 26   | compactness(worst) - 致密性（最大值）                  | 0.027  | 1.058  |
| 27   | concavity(worst) - 凹度（最大值）                      | 0.0    | 1.252  |
| 28   | concave points(worst) - 凹点（最大值）                 | 0.0    | 0.291  |
| 29   | symmetry(worst) - 对称性（最大值）                     | 0.156  | 0.664  |
| 30   | fractal dimension(worst) - 分形维数（最大值）          | 0.055  | 0.208  |

**目标分类分布：** $212$ - 恶性（Malignant），$357$ - 良性（Benign）

```python
from sklearn.datasets import load_breast_cancer

# 加载sklearn自带的乳腺癌数据集
dataset = load_breast_cancer()

# 提取特征数据和目标数据，都是 numpy.ndarray 类型
X = dataset.data
y = dataset.target
```

## 预处理和规范化

| 预处理和规范化                                               | 作用                                                   |
| :----------------------------------------------------------- | :----------------------------------------------------- |
| [`preprocessing.Binarizer`](https://scikit-learn.org.cn/view/721.html)(*[, threshold, copy]) | 根据阈值对数据进行二值化（将要素值设置为0或1）         |
| [`preprocessing.KBinsDiscretizer`](https://scikit-learn.org.cn/view/722.html)（[n_bins，...]） | 将连续数据分成间隔。                                   |
| [`preprocessing.LabelEncoder`](https://scikit-learn.org.cn/view/730.html) | 使用 $0$ 到 n_classes-$1$ 之间的值对目标标签进行编码。 |
| [`preprocessing.MinMaxScaler`](https://scikit-learn.org.cn/view/733.html)([feature_range, copy]) | 通过将每个要素缩放到给定范围来变换要素。               |
| [`preprocessing.OneHotEncoder`](https://scikit-learn.org.cn/view/740.html)(*[, categories, …]) | 将分类要素编码为一键式数字数组。                       |
| [`preprocessing.StandardScaler`](https://scikit-learn.org.cn/view/753.html)(*[, copy, …]) | 通过去除均值并缩放到单位方差来标准化特征               |

> 更多参考：[API 参考-scikit-learn中文社区](https://scikit-learn.org.cn/lists/3.html#sklearn.preprocessing：预处理和规范化))

泰坦尼克数据字段描述如下：

| 英文字段名  | 中文含义                 | 值说明                             |
| ----------- | ------------------------ | ---------------------------------- |
| survived    | 是否生还，数值类型       | $0$，未生还；$1$ 生还              |
| pclass      | 客舱等级，数值类型       | $1$、$2$、$3$ 对应一、二、三等舱   |
| sex         | 性别，字符类型           | male，男性；female 女性            |
| age         | 年龄，浮点型             | 存在 177 条记录缺失该字段值        |
| sibsp       | 船上的兄弟姐妹的人数     | 整数类型                           |
| parch       | 船上父母和孩子人数       | 整数类型                           |
| fare        | 票价，浮点型             | 浮点型                             |
| embarked    | 登录港口，字符类型       | S、C、Q                            |
| class       | 客舱等级，字符类型       | First、Second、Third               |
| who         | 类型，字符类型           | man、woman、child                  |
| adult_male  | 是否成年男性，布尔类型   | True、False                        |
| deck        | 舱面，字符类型           | A、C、D、E、G                      |
| embark_town | 登录港口全名，字符类型   | Cherbourg、Queenstown、Southampton |
| alive       | 是否生还，字符类型       | yes，生还；no 死亡                 |
| alone       | 是否单独一个人，布尔类型 | True、False                        |

> 思考一下，上面这样的数据是否要处理，应该怎么处理？

```python
import pandas as pd
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.preprocessing import MinMaxScaler

# 获取数据
df = sns.load_dataset('titanic')

# 设置显示所有的的列 jupyter
pd.set_option('display.max_columns', None)

# 输出数据集
print(df.head(30))

# 处理冗余数据
df.drop(columns=['class'], inplace=True)
df.drop(columns=['alive'], inplace=True)
df.drop(columns=['embarked'], inplace=True)

# 统计缺失值情况
print(df.isnull().sum())

# 处理缺失值 - 使用均值填充 age（年龄）列的缺失值
imputer = SimpleImputer(strategy='mean')
df['age'] = imputer.fit_transform(df[['age']])

# 处理缺失值 - 使用众数填充 embark_town（登录港口）列的缺失值
imputer = SimpleImputer(strategy='most_frequent')
df['embark_town'] = imputer.fit_transform(df[['embark_town']])

# 处理缺失值 - 缺失太多直接删除 deck（舱面）列
df.drop(columns=['deck'], inplace=True)

# 处理类别特征 - 使用 LabelEncoder 对 adult_male（成人男性）进行整数编码
label_encoder = LabelEncoder()
df['adult_male'] = label_encoder.fit_transform(df['adult_male'])

# 处理类别特征 - 使用 LabelEncoder 对 alone（单独）进行整数编码
label_encoder = LabelEncoder()
df['alone'] = label_encoder.fit_transform(df['alone'])

# 处理类别特征 - 使用 OneHotEncoder 对 sex（性别）进行独热编码
onehot_encoder = OneHotEncoder(sparse=False)
encoded_features = onehot_encoder.fit_transform(df[['sex']])
df= pd.concat([df, pd.DataFrame(encoded_features, columns=['female', 'male'])], axis=1)
df.drop(columns=['sex'], inplace=True)

# 处理类别特征 - 使用 OneHotEncoder 对 who（类型）进行独热编码
onehot_encoder = OneHotEncoder(sparse=False)
encoded_features = onehot_encoder.fit_transform(df[['who']])
df= pd.concat([df, pd.DataFrame(encoded_features, columns=['child', 'man', 'woman'])], axis=1)
df.drop(columns=['who'], inplace=True)

# 处理类别特征 - 使用 OneHotEncoder 对 embark_town（登录港口）进行独热编码
onehot_encoder = OneHotEncoder(sparse=False)
encoded_features = onehot_encoder.fit_transform(df[['embark_town']])
df= pd.concat([df, pd.DataFrame(encoded_features, columns=['Cherbourg', 'Queenstown', 'Southampton'])], axis=1)
df.drop(columns=['embark_town'], inplace=True)

# 特征缩放 - 使用 MinMaxScaler 对 Fare（客票价）列进行缩放
scaler = MinMaxScaler()
df['fare_scaled'] = scaler.fit_transform(df[['fare']])
df.drop(columns=['fare'], inplace=True)

# 输出处理后的数据集
print(df.head(30))
```

# 波士顿房价

## 需求

波士顿房价预测是一个经典的机器学习任务，类似于程序员世界的 “Hello World”。和大家对房价的普遍认知相同，波士顿地区的房价受诸多因素影响。该数据集统计了 $13$ 种可能影响房价的因素和该类型房屋的均价，期望构建一个基于 $13$ 个因素进行房价预测的模型，如下图所示：

![波士顿房价影响因素](https://markdown-hesj.oss-cn-guangzhou.aliyuncs.com/%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0.png)

对于预测问题，可以根据预测输出的类型是连续的实数值，还是离散的标签，区分为回归任务和分类任务。因为房价是一个连续值，所以房价预测显然是一个回归任务。下面我们尝试用最简单的线性回归模型解决这个问题。

## 代码实现

```python
from sklearn.datasets import load_boston
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# 加载波士顿房价数据集
boston = load_boston()
X = boston.data
y = boston.target

# 特征缩放 - Min-Max标准化
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# 将数据集划分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.125, random_state=42)

# 创建线性回归模型
model = LinearRegression()
# model = SGDRegressor(learning_rate='constant', eta0=0.01, max_iter=5000)

# 在训练集上拟合模型
model.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = model.predict(X_test)

# 计算均方误差（Mean Squared Error）
mse = mean_squared_error(y_test, y_pred)
print("均方误差（MSE）：", mse)
```

# 鸢尾花

## 需求

Iris 也称鸢尾花卉数据集，是一类多重变量分析的数据集。数据集包含 $150$ 个数据集，分为 $3$ 类，每类 $50$ 个数据，每个数据包含 $4$ 个属性。

现在我们希望通过花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性预测鸢尾花卉属于（Setosa，Versicolour，Virginica）三个种类中的哪一类。

## 代码实现

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data
y = iris.target

# 将数据集划分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.125, random_state=42)

# 创建逻辑回归模型
model = LogisticRegression(max_iter=1000)

# 在训练集上拟合模型
model.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = model.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print("准确率：", accuracy)
```

## 常用评估指标

在分类任务中，准确率、精确率、召回率和 F1 分数是评估模型性能的常用指标，它们从不同的角度衡量了模型的预测质量。

假设我们有一个简单的二分类问题，目标是识别邮件是否为垃圾邮件（$1$ 表示垃圾邮件，$0$ 表示非垃圾邮件）。我们的模型在一个测试数据集上的预测结果如下：

| 实际\预测        | 非垃圾邮件 ($0$) | 垃圾邮件 ($1$) |
| ---------------- | ---------------- | -------------- |
| 非垃圾邮件 ($0$) | **$90$（TN）**   | $10$（FP）     |
| 垃圾邮件 ($1$)   | $30$（FN）       | **$70$（TP）** |

在这个例子中：

- 真正例（TP）：实际为垃圾邮件且正确预测为垃圾邮件的邮件数量为 $70$
- 假正例（FP）：实际为非垃圾邮件但错误预测为垃圾邮件的邮件数量为 $10$
- 真负例（TN）：实际为非垃圾邮件且正确预测为非垃圾邮件的邮件数量为 $90$
- 假负例（FN）：实际为垃圾邮件但错误预测为非垃圾邮件的邮件数量为 $30$

现在我们可以计算准确率、精确率和召回率了：

准确率（Accuracy）是所有分类正确的观测数占总观测数的比例。 
$$
准确率 = \frac{TP+TN}{TP + FP + TN + FN} = \frac{70 + 90}{70 + 10 + 90 + 30}= 0.8
$$
精确率（Precision）是真正例占所有预测为正类的观测的比例，误报（假正例）的数量很少。
$$
精确率 = \frac{TP}{TP + FP} = \frac{70}{70 + 10} = 0.875
$$
召回率（Recall）是在所有实际为正的样本中，模型正确预测为正的比例，高意味遗漏（假负例）的数量很少。
$$
召回率 = \frac{TP}{TP + FN} =\frac{70}{70 + 30} = 0.7
$$
F1 分数（F1 Score）是精确率和召回率的调和平均。 
$$
F1 = 2 \times \frac{精确率 \times 召回率}{精确率 + 召回率} = 2 \times \frac{0.875 \times 0.7}{0.875 + 0.7} = 0.778
$$
当各类别基本平衡时，准确率是一个直观且有用的性能指标。但在类别不平衡的情况下，希望少误报使用精确率，希望少遗漏使用召回率。而 F1 分数在处理不平衡数据集时特别有用，因为它同时考虑了精确率和召回率，避免了仅依赖单一指标可能导致的偏见。总的来说，准确率、召回率和 F1 分数各自从不同的角度评估了模型的预测性能，选择哪个指标取决于具体的应用场景和目标。

# 乳腺癌

## 需求

Scikit-learn 内置的乳腺癌数据集来自加州大学欧文分校机器学习仓库中的威斯康辛州乳腺癌数据集。乳腺癌数据集是一个共有 $569$ 个样本、$30$ 个输入变量和 $2$ 个分类的数据集。

现在我们希望通过这些数据训练出一个模型，能识别出乳腺癌是良性的，还是恶性的。

## 代码实现

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# 加载数据集
data = load_breast_cancer()
X, y = data.data, data.target

# 划分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 选择一个模型来训练
model = RandomForestClassifier(n_estimators=100, random_state=42)

# 训练模型
model.fit(X_train, y_train)

# 预测测试集结果
y_pred = model.predict(X_test)

# 计算并打印准确度
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
# 你可以打印更多性能指标，如混淆矩阵、召回率、F1分数等
```

